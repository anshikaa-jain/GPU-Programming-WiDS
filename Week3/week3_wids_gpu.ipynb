{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cVJGyENZO8bS",
        "outputId": "fe805b41-104b-4a3b-bf5d-13de96f13c0f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing coalesced.cu\n"
          ]
        }
      ],
      "source": [
        "%%writefile coalesced.cu\n",
        "#include <cuda_runtime.h>\n",
        "#include <iostream>\n",
        "\n",
        "__global__ void coalescedKernel(const float* A, const float* B, float* C, int N) {\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    // Bounds check\n",
        "    if (idx < N) {\n",
        "        C[idx] = A[idx] + B[idx];\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int N = 1 << 24;  // ~16 million elements\n",
        "    size_t size = N * sizeof(float);\n",
        "\n",
        "    float *hA = new float[N];\n",
        "    float *hB = new float[N];\n",
        "    float *hC = new float[N];\n",
        "\n",
        "    for (int i = 0; i < N; i++) {\n",
        "        hA[i] = 1.0f;\n",
        "        hB[i] = 2.0f;\n",
        "    }\n",
        "\n",
        "    float *dA, *dB, *dC;\n",
        "    cudaMalloc(&dA, size);\n",
        "    cudaMalloc(&dB, size);\n",
        "    cudaMalloc(&dC, size);\n",
        "\n",
        "    cudaMemcpy(dA, hA, size, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(dB, hB, size, cudaMemcpyHostToDevice);\n",
        "\n",
        "    dim3 block(256);\n",
        "    dim3 grid((N + block.x - 1) / block.x);\n",
        "\n",
        "    cudaEvent_t start, stop;\n",
        "    cudaEventCreate(&start);\n",
        "    cudaEventCreate(&stop);\n",
        "\n",
        "    cudaEventRecord(start);\n",
        "    coalescedKernel<<<grid, block>>>(dA, dB, dC, N);\n",
        "    cudaEventRecord(stop);\n",
        "\n",
        "    cudaEventSynchronize(stop);\n",
        "\n",
        "    float ms;\n",
        "    cudaEventElapsedTime(&ms, start, stop);\n",
        "    std::cout << \"Coalesced kernel time: \" << ms << \" ms\\n\";\n",
        "\n",
        "    cudaFree(dA);\n",
        "    cudaFree(dB);\n",
        "    cudaFree(dC);\n",
        "    delete[] hA;\n",
        "    delete[] hB;\n",
        "    delete[] hC;\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc coalesced.cu -o coalesced \\\n",
        "  -gencode arch=compute_75,code=sm_75 \\\n",
        "  -Xptxas -v\n",
        "\n",
        "!./coalesced\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qaShVZbiSVT9",
        "outputId": "268f0232-4fa2-4f00-d00d-4513d2e0c73c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ptxas info    : 0 bytes gmem\n",
            "ptxas info    : Compiling entry function '_Z15coalescedKernelPKfS0_Pfi' for 'sm_75'\n",
            "ptxas info    : Function properties for _Z15coalescedKernelPKfS0_Pfi\n",
            "    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "ptxas info    : Used 12 registers, 380 bytes cmem[0]\n",
            "Coalesced kernel time: 0.880576 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile non_coalesced.cu\n",
        "#include <cuda_runtime.h>\n",
        "#include <iostream>\n",
        "\n",
        "#define STRIDE 8\n",
        "\n",
        "__global__ void nonCoalescedKernel(const float* A, const float* B, float* C, int N) {\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    int accessIdx = idx * STRIDE;\n",
        "\n",
        "    if (accessIdx < N) {\n",
        "        C[accessIdx] = A[accessIdx] + B[accessIdx];\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int N = 1 << 24;\n",
        "    size_t size = N * sizeof(float);\n",
        "\n",
        "    float *hA = new float[N];\n",
        "    float *hB = new float[N];\n",
        "    float *hC = new float[N];\n",
        "\n",
        "    for (int i = 0; i < N; i++) {\n",
        "        hA[i] = 1.0f;\n",
        "        hB[i] = 2.0f;\n",
        "    }\n",
        "\n",
        "    float *dA, *dB, *dC;\n",
        "    cudaMalloc(&dA, size);\n",
        "    cudaMalloc(&dB, size);\n",
        "    cudaMalloc(&dC, size);\n",
        "\n",
        "    cudaMemcpy(dA, hA, size, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(dB, hB, size, cudaMemcpyHostToDevice);\n",
        "\n",
        "    dim3 block(256);\n",
        "    dim3 grid((N / STRIDE + block.x - 1) / block.x);\n",
        "\n",
        "    cudaEvent_t start, stop;\n",
        "    cudaEventCreate(&start);\n",
        "    cudaEventCreate(&stop);\n",
        "\n",
        "    cudaEventRecord(start);\n",
        "    nonCoalescedKernel<<<grid, block>>>(dA, dB, dC, N);\n",
        "    cudaEventRecord(stop);\n",
        "\n",
        "    cudaEventSynchronize(stop);\n",
        "\n",
        "    float ms;\n",
        "    cudaEventElapsedTime(&ms, start, stop);\n",
        "    std::cout << \"Non-coalesced kernel time: \" << ms << \" ms\\n\";\n",
        "\n",
        "    cudaFree(dA);\n",
        "    cudaFree(dB);\n",
        "    cudaFree(dC);\n",
        "    delete[] hA;\n",
        "    delete[] hB;\n",
        "    delete[] hC;\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "id": "QkUtXFF9Ql9n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bde81225-5670-48fd-eeee-974d613c90a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing non_coalesced.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc non_coalesced.cu -o non_coalesced \\\n",
        "  -gencode arch=compute_75,code=sm_75 \\\n",
        "  -Xptxas -v\n",
        "\n",
        "!./non_coalesced"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tvAIjVvXSqy5",
        "outputId": "4b441f58-586c-4767-b486-4581c206622c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ptxas info    : 0 bytes gmem\n",
            "ptxas info    : Compiling entry function '_Z18nonCoalescedKernelPKfS0_Pfi' for 'sm_75'\n",
            "ptxas info    : Function properties for _Z18nonCoalescedKernelPKfS0_Pfi\n",
            "    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "ptxas info    : Used 12 registers, 380 bytes cmem[0]\n",
            "Non-coalesced kernel time: 1.10883 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile shared_memory.cu\n",
        "#include <cuda_runtime.h>\n",
        "#include <iostream>\n",
        "\n",
        "#define BLOCK_SIZE 256\n",
        "\n",
        "// Baseline: Global Memory Reduction\n",
        "__global__ void globalReduce(const float* input, float* output, int N) {\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    if (idx < N) {\n",
        "        atomicAdd(output, input[idx]);\n",
        "    }\n",
        "}\n",
        "\n",
        "// Optimized: Shared Memory Reduction\n",
        "\n",
        "__global__ void sharedReduce(const float* input, float* output, int N) {\n",
        "    __shared__ float sdata[BLOCK_SIZE];\n",
        "\n",
        "    int tid = threadIdx.x;\n",
        "    int idx = blockIdx.x * blockDim.x + tid;\n",
        "\n",
        "    // Load from global memory to shared memory\n",
        "    if (idx < N)\n",
        "        sdata[tid] = input[idx];\n",
        "    else\n",
        "        sdata[tid] = 0.0f;\n",
        "\n",
        "    __syncthreads();\n",
        "\n",
        "    // Reduction in shared memory\n",
        "    for (int stride = blockDim.x / 2; stride > 0; stride >>= 1) {\n",
        "        if (tid < stride) {\n",
        "            sdata[tid] += sdata[tid + stride];\n",
        "        }\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    // One atomic add per block\n",
        "    if (tid == 0) {\n",
        "        atomicAdd(output, sdata[0]);\n",
        "    }\n",
        "}\n",
        "\n",
        "// Host Code\n",
        "int main() {\n",
        "    int N = 1 << 20;  // ~1 million elements\n",
        "    size_t size = N * sizeof(float);\n",
        "\n",
        "    float* h_input = new float[N];\n",
        "    for (int i = 0; i < N; i++)\n",
        "        h_input[i] = 1.0f;\n",
        "\n",
        "    float *d_input, *d_output;\n",
        "    cudaMalloc(&d_input, size);\n",
        "    cudaMalloc(&d_output, sizeof(float));\n",
        "\n",
        "    cudaMemcpy(d_input, h_input, size, cudaMemcpyHostToDevice);\n",
        "\n",
        "    dim3 block(BLOCK_SIZE);\n",
        "    dim3 grid((N + BLOCK_SIZE - 1) / BLOCK_SIZE);\n",
        "\n",
        "    // ---------------- Global Memory Version ----------------\n",
        "    cudaMemset(d_output, 0, sizeof(float));\n",
        "\n",
        "    cudaEvent_t start1, stop1;\n",
        "    cudaEventCreate(&start1);\n",
        "    cudaEventCreate(&stop1);\n",
        "\n",
        "    cudaEventRecord(start1);\n",
        "    globalReduce<<<grid, block>>>(d_input, d_output, N);\n",
        "    cudaEventRecord(stop1);\n",
        "\n",
        "    cudaEventSynchronize(stop1);\n",
        "\n",
        "    float time_global;\n",
        "    cudaEventElapsedTime(&time_global, start1, stop1);\n",
        "\n",
        "    // ---------------- Shared Memory Version ----------------\n",
        "    cudaMemset(d_output, 0, sizeof(float));\n",
        "\n",
        "    cudaEvent_t start2, stop2;\n",
        "    cudaEventCreate(&start2);\n",
        "    cudaEventCreate(&stop2);\n",
        "\n",
        "    cudaEventRecord(start2);\n",
        "    sharedReduce<<<grid, block>>>(d_input, d_output, N);\n",
        "    cudaEventRecord(stop2);\n",
        "\n",
        "    cudaEventSynchronize(stop2);\n",
        "\n",
        "    float time_shared;\n",
        "    cudaEventElapsedTime(&time_shared, start2, stop2);\n",
        "\n",
        "    // ---------------- Results ----------------\n",
        "    std::cout << \"Global memory kernel time:  \" << time_global << \" ms\\n\";\n",
        "    std::cout << \"Shared memory kernel time:  \" << time_shared << \" ms\\n\";\n",
        "    std::cout << \"Speedup: \" << time_global / time_shared << \"x\\n\";\n",
        "\n",
        "    cudaFree(d_input);\n",
        "    cudaFree(d_output);\n",
        "    delete[] h_input;\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dxgRCOoJQms_",
        "outputId": "045cacc9-7f5c-402a-f41e-a0eb4e71f187"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing shared_memory.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc shared_memory.cu -o shared_memory \\\n",
        "  -gencode arch=compute_75,code=sm_75 \\\n",
        "  -Xptxas -v\n",
        "\n",
        "!./shared_memory"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "My8q4j2rTG_A",
        "outputId": "192aedf1-80dc-49e4-defb-f4f3760bba98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ptxas info    : 0 bytes gmem\n",
            "ptxas info    : Compiling entry function '_Z12sharedReducePKfPfi' for 'sm_75'\n",
            "ptxas info    : Function properties for _Z12sharedReducePKfPfi\n",
            "    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "ptxas info    : Used 10 registers, 1024 bytes smem, 372 bytes cmem[0]\n",
            "ptxas info    : Compiling entry function '_Z12globalReducePKfPfi' for 'sm_75'\n",
            "ptxas info    : Function properties for _Z12globalReducePKfPfi\n",
            "    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "ptxas info    : Used 6 registers, 372 bytes cmem[0]\n",
            "Global memory kernel time:  3.77238 ms\n",
            "Shared memory kernel time:  0.122784 ms\n",
            "Speedup: 30.7237x\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# #cpu_baseline\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import time\n",
        "\n",
        "# # -------------------------------------------------\n",
        "# # Force CPU\n",
        "# # -------------------------------------------------\n",
        "# device = torch.device(\"cpu\")\n",
        "\n",
        "# # -------------------------------------------------\n",
        "# # Tiny U-Net Model\n",
        "# # -------------------------------------------------\n",
        "# class TinyUNet(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super().__init__()\n",
        "\n",
        "#         self.enc1 = nn.Sequential(\n",
        "#             nn.Conv2d(1, 8, 3, padding=1),\n",
        "#             nn.ReLU(),\n",
        "#             nn.Conv2d(8, 8, 3, padding=1),\n",
        "#             nn.ReLU()\n",
        "#         )\n",
        "\n",
        "#         self.pool = nn.MaxPool2d(2)\n",
        "\n",
        "#         self.bottleneck = nn.Sequential(\n",
        "#             nn.Conv2d(8, 16, 3, padding=1),\n",
        "#             nn.ReLU(),\n",
        "#             nn.Conv2d(16, 16, 3, padding=1),\n",
        "#             nn.ReLU()\n",
        "#         )\n",
        "\n",
        "#         self.up = nn.Upsample(scale_factor=2, mode='nearest')\n",
        "\n",
        "#         self.dec1 = nn.Sequential(\n",
        "#             nn.Conv2d(16, 8, 3, padding=1),\n",
        "#             nn.ReLU(),\n",
        "#             nn.Conv2d(8, 8, 3, padding=1),\n",
        "#             nn.ReLU()\n",
        "#         )\n",
        "\n",
        "#         self.out = nn.Conv2d(8, 1, 1)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x1 = self.enc1(x)\n",
        "#         x2 = self.pool(x1)\n",
        "#         x3 = self.bottleneck(x2)\n",
        "#         x4 = self.up(x3)\n",
        "#         x5 = self.dec1(x4)\n",
        "#         return self.out(x5)\n",
        "\n",
        "# # -------------------------------------------------\n",
        "# # Input: Grayscale Medical Image\n",
        "# # -------------------------------------------------\n",
        "# input_image = torch.randn(1, 1, 256, 256, device=device)\n",
        "\n",
        "# model = TinyUNet().to(device)\n",
        "# model.eval()\n",
        "\n",
        "# # -------------------------------------------------\n",
        "# # CPU Timing\n",
        "# # -------------------------------------------------\n",
        "# with torch.no_grad():\n",
        "#     start = time.time()\n",
        "#     output = model(input_image)\n",
        "#     end = time.time()\n",
        "\n",
        "# print(f\"CPU inference time: {end - start:.4f} seconds\")\n",
        "# print(f\"Output shape: {output.shape}\")\n"
      ],
      "metadata": {
        "id": "IZ-tt-XeQ_bB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #gpu_acc\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import time\n",
        "\n",
        "# # -------------------------------------------------\n",
        "# # GPU Device\n",
        "# # -------------------------------------------------\n",
        "# device = torch.device(\"cuda\")\n",
        "\n",
        "# # -------------------------------------------------\n",
        "# # Tiny U-Net Model (UNCHANGED)\n",
        "# # -------------------------------------------------\n",
        "# class TinyUNet(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super().__init__()\n",
        "\n",
        "#         self.enc1 = nn.Sequential(\n",
        "#             nn.Conv2d(1, 8, 3, padding=1),\n",
        "#             nn.ReLU(),\n",
        "#             nn.Conv2d(8, 8, 3, padding=1),\n",
        "#             nn.ReLU()\n",
        "#         )\n",
        "\n",
        "#         self.pool = nn.MaxPool2d(2)\n",
        "\n",
        "#         self.bottleneck = nn.Sequential(\n",
        "#             nn.Conv2d(8, 16, 3, padding=1),\n",
        "#             nn.ReLU(),\n",
        "#             nn.Conv2d(16, 16, 3, padding=1),\n",
        "#             nn.ReLU()\n",
        "#         )\n",
        "\n",
        "#         self.up = nn.Upsample(scale_factor=2, mode='nearest')\n",
        "\n",
        "#         self.dec1 = nn.Sequential(\n",
        "#             nn.Conv2d(16, 8, 3, padding=1),\n",
        "#             nn.ReLU(),\n",
        "#             nn.Conv2d(8, 8, 3, padding=1),\n",
        "#             nn.ReLU()\n",
        "#         )\n",
        "\n",
        "#         self.out = nn.Conv2d(8, 1, 1)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x1 = self.enc1(x)\n",
        "#         x2 = self.pool(x1)\n",
        "#         x3 = self.bottleneck(x2)\n",
        "#         x4 = self.up(x3)\n",
        "#         x5 = self.dec1(x4)\n",
        "#         return self.out(x5)\n",
        "\n",
        "# # -------------------------------------------------\n",
        "# # Input Image\n",
        "# # -------------------------------------------------\n",
        "# input_image = torch.randn(1, 1, 256, 256, device=device)\n",
        "\n",
        "# model = TinyUNet().to(device)\n",
        "# model.eval()\n",
        "\n",
        "# # Warm-up (important for fair GPU timing)\n",
        "# with torch.no_grad():\n",
        "#     for _ in range(5):\n",
        "#         _ = model(input_image)\n",
        "\n",
        "# torch.cuda.synchronize()\n",
        "\n",
        "# # -------------------------------------------------\n",
        "# # GPU Timing (CUDA events)\n",
        "# # -------------------------------------------------\n",
        "# start = torch.cuda.Event(enable_timing=True)\n",
        "# end = torch.cuda.Event(enable_timing=True)\n",
        "\n",
        "# with torch.no_grad():\n",
        "#     start.record()\n",
        "#     output = model(input_image)\n",
        "#     end.record()\n",
        "\n",
        "# torch.cuda.synchronize()\n",
        "\n",
        "# print(f\"GPU inference time: {start.elapsed_time(end):.4f} ms\")\n",
        "# print(f\"Output shape: {output.shape}\")\n"
      ],
      "metadata": {
        "id": "LHlC-FqXROiD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import time\n",
        "\n",
        "# Dimensions\n",
        "H, W = 256, 256\n",
        "C1, C2 = 4, 8\n",
        "K = 3\n",
        "\n",
        "# Input image\n",
        "input_img = np.ones((1, H, W), dtype=np.float32)\n",
        "\n",
        "# Weights\n",
        "W1 = np.random.randn(C1, 1, K, K).astype(np.float32)\n",
        "W2 = np.random.randn(C2, C1, K, K).astype(np.float32)\n",
        "W3 = np.random.randn(C1, C2, K, K).astype(np.float32)\n",
        "\n",
        "# Helper functions\n",
        "def conv_relu(inp, W):\n",
        "    C_out, C_in, _, _ = W.shape\n",
        "    _, H, Wd = inp.shape\n",
        "    out = np.zeros((C_out, H, Wd), dtype=np.float32)\n",
        "\n",
        "    for oc in range(C_out):\n",
        "        for y in range(H):\n",
        "            for x in range(Wd):\n",
        "                acc = 0.0\n",
        "                for ic in range(C_in):\n",
        "                    for ky in range(-1, 2):\n",
        "                        for kx in range(-1, 2):\n",
        "                            iy, ix = y + ky, x + kx\n",
        "                            if 0 <= iy < H and 0 <= ix < Wd:\n",
        "                                acc += inp[ic, iy, ix] * W[oc, ic, ky+1, kx+1]\n",
        "                out[oc, y, x] = max(acc, 0.0)\n",
        "    return out\n",
        "\n",
        "def maxpool(inp):\n",
        "    C, H, W = inp.shape\n",
        "    out = np.zeros((C, H//2, W//2), dtype=np.float32)\n",
        "    for c in range(C):\n",
        "        for y in range(0, H, 2):\n",
        "            for x in range(0, W, 2):\n",
        "                out[c, y//2, x//2] = np.max(inp[c, y:y+2, x:x+2])\n",
        "    return out\n",
        "\n",
        "def upsample(inp):\n",
        "    C, H, W = inp.shape\n",
        "    out = np.zeros((C, H*2, W*2), dtype=np.float32)\n",
        "    for c in range(C):\n",
        "        for y in range(H):\n",
        "            for x in range(W):\n",
        "                out[c, y*2:y*2+2, x*2:x*2+2] = inp[c, y, x]\n",
        "    return out\n",
        "\n",
        "\n",
        "# CPU Timing\n",
        "start = time.time()\n",
        "\n",
        "enc1 = conv_relu(input_img, W1)\n",
        "skip = enc1.copy()\n",
        "enc2 = maxpool(enc1)\n",
        "enc3 = conv_relu(enc2, W2)\n",
        "dec1 = upsample(enc3)\n",
        "dec2 = conv_relu(dec1, W3)\n",
        "output = dec2 + skip\n",
        "\n",
        "end = time.time()\n",
        "\n",
        "print(f\"[CPU] U-Net forward time: {end - start:.3f} s\")\n",
        "print(\"Output shape:\", output.shape)\n"
      ],
      "metadata": {
        "id": "XEY1-bRBRT14",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f933c91-769d-4074-e370-d7c230220843"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CPU] U-Net forward time: 16.587 s\n",
            "Output shape: (4, 256, 256)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile gpu1.cu\n",
        "#include <cuda_runtime.h>\n",
        "#include <iostream>\n",
        "\n",
        "#define H 256\n",
        "#define W 256\n",
        "#define C1 4\n",
        "#define C2 8\n",
        "#define BX 16\n",
        "#define BY 16\n",
        "\n",
        "// --------------------------------------------------\n",
        "// Conv + ReLU kernel\n",
        "// --------------------------------------------------\n",
        "__global__ void conv_relu(\n",
        "    const float* input,\n",
        "    const float* weight,\n",
        "    float* output,\n",
        "    int Ht, int Wt, int Cin, int Cout\n",
        ") {\n",
        "    int x = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    int y = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "    int oc = blockIdx.z;\n",
        "\n",
        "    if (x >= Wt || y >= Ht) return;\n",
        "\n",
        "    float sum = 0.0f;\n",
        "\n",
        "    for (int ic = 0; ic < Cin; ic++) {\n",
        "        for (int ky = -1; ky <= 1; ky++) {\n",
        "            for (int kx = -1; kx <= 1; kx++) {\n",
        "                int ix = x + kx;\n",
        "                int iy = y + ky;\n",
        "                if (ix >= 0 && ix < Wt && iy >= 0 && iy < Ht) {\n",
        "                    float val = input[ic * Ht * Wt + iy * Wt + ix];\n",
        "                    float w = weight[\n",
        "                        oc * Cin * 9 +\n",
        "                        ic * 9 +\n",
        "                        (ky + 1) * 3 + (kx + 1)\n",
        "                    ];\n",
        "                    sum += val * w;\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "    output[oc * Ht * Wt + y * Wt + x] = (sum > 0.0f) ? sum : 0.0f;\n",
        "}\n",
        "\n",
        "// --------------------------------------------------\n",
        "// 2Ã—2 MaxPool\n",
        "// --------------------------------------------------\n",
        "__global__ void maxpool(\n",
        "    const float* input,\n",
        "    float* output,\n",
        "    int Cin\n",
        ") {\n",
        "    int x = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    int y = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "    int c = blockIdx.z;\n",
        "\n",
        "    int ix = x * 2;\n",
        "    int iy = y * 2;\n",
        "\n",
        "    if (ix + 1 >= W || iy + 1 >= H) return;\n",
        "\n",
        "    float m = input[c * H * W + iy * W + ix];\n",
        "    m = fmaxf(m, input[c * H * W + iy * W + ix + 1]);\n",
        "    m = fmaxf(m, input[c * H * W + (iy + 1) * W + ix]);\n",
        "    m = fmaxf(m, input[c * H * W + (iy + 1) * W + ix + 1]);\n",
        "\n",
        "    output[c * (H/2) * (W/2) + y * (W/2) + x] = m;\n",
        "}\n",
        "\n",
        "// --------------------------------------------------\n",
        "// Upsampling (nearest neighbor)\n",
        "// --------------------------------------------------\n",
        "__global__ void upsample(\n",
        "    const float* input,\n",
        "    float* output,\n",
        "    int Cin\n",
        ") {\n",
        "    int x = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    int y = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "    int c = blockIdx.z;\n",
        "\n",
        "    if (x >= W || y >= H) return;\n",
        "\n",
        "    output[c * H * W + y * W + x] =\n",
        "        input[c * (H/2) * (W/2) + (y/2) * (W/2) + (x/2)];\n",
        "}\n",
        "\n",
        "// --------------------------------------------------\n",
        "// MAIN\n",
        "// --------------------------------------------------\n",
        "int main() {\n",
        "\n",
        "    size_t s1 = 1 * H * W * sizeof(float);\n",
        "    size_t s4 = C1 * H * W * sizeof(float);\n",
        "    size_t s8 = C2 * (H/2) * (W/2) * sizeof(float);\n",
        "\n",
        "    float *d_in, *d_e1, *d_skip, *d_p, *d_e2, *d_up, *d_out;\n",
        "    float *w1, *w2, *w3;\n",
        "\n",
        "    cudaMalloc(&d_in, s1);\n",
        "    cudaMalloc(&d_e1, s4);\n",
        "    cudaMalloc(&d_skip, s4);\n",
        "    cudaMalloc(&d_p, C1 * (H/2) * (W/2) * sizeof(float));\n",
        "    cudaMalloc(&d_e2, s8);\n",
        "    cudaMalloc(&d_up, s4);\n",
        "    cudaMalloc(&d_out, s4);\n",
        "\n",
        "    cudaMalloc(&w1, C1 * 9 * sizeof(float));\n",
        "    cudaMalloc(&w2, C2 * C1 * 9 * sizeof(float));\n",
        "    cudaMalloc(&w3, C1 * C2 * 9 * sizeof(float));\n",
        "\n",
        "    cudaMemset(d_in, 1, s1);\n",
        "\n",
        "    dim3 block(BX, BY);\n",
        "    dim3 grid1((W + BX - 1) / BX, (H + BY - 1) / BY, C1);\n",
        "    dim3 grid2((W/2 + BX - 1) / BX, (H/2 + BY - 1) / BY, C2);\n",
        "\n",
        "    // ---------------- Warm-up ----------------\n",
        "    conv_relu<<<grid1, block>>>(d_in, w1, d_e1, H, W, 1, C1);\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    // ---------------- Timing ----------------\n",
        "    cudaEvent_t start, stop;\n",
        "    cudaEventCreate(&start);\n",
        "    cudaEventCreate(&stop);\n",
        "\n",
        "    cudaEventRecord(start);\n",
        "\n",
        "    // Encoder\n",
        "    conv_relu<<<grid1, block>>>(d_in, w1, d_e1, H, W, 1, C1);\n",
        "    cudaMemcpy(d_skip, d_e1, s4, cudaMemcpyDeviceToDevice);\n",
        "\n",
        "    maxpool<<<dim3((W/2+BX-1)/BX, (H/2+BY-1)/BY, C1), block>>>(d_e1, d_p, C1);\n",
        "\n",
        "    conv_relu<<<grid2, block>>>(d_p, w2, d_e2, H/2, W/2, C1, C2);\n",
        "\n",
        "    // Decoder\n",
        "    upsample<<<grid1, block>>>(d_e2, d_up, C2);\n",
        "    conv_relu<<<grid1, block>>>(d_up, w3, d_out, H, W, C2, C1);\n",
        "\n",
        "    cudaEventRecord(stop);\n",
        "    cudaEventSynchronize(stop);\n",
        "\n",
        "    float time_ms;\n",
        "    cudaEventElapsedTime(&time_ms, start, stop);\n",
        "\n",
        "    std::cout << \"[GPU] U-Net forward pass time: \"\n",
        "              << time_ms << \" ms\\n\";\n",
        "\n",
        "    cudaFree(d_in); cudaFree(d_e1); cudaFree(d_skip);\n",
        "    cudaFree(d_p);  cudaFree(d_e2); cudaFree(d_up); cudaFree(d_out);\n",
        "    cudaFree(w1); cudaFree(w2); cudaFree(w3);\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "exw-kHBxWn6O",
        "outputId": "54520d73-67b8-4e9c-dc85-77051481e4dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing gpu1.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc gpu1.cu -o gpu1 \\\n",
        "  -gencode arch=compute_75,code=sm_75 \\\n",
        "  -Xptxas -v\n",
        "\n",
        "!./gpu1"
      ],
      "metadata": {
        "id": "n7zQoDObXNFM",
        "outputId": "54436bd7-7f37-4500-f189-0dc4566e4a4e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ptxas info    : 0 bytes gmem\n",
            "ptxas info    : Compiling entry function '_Z8upsamplePKfPfi' for 'sm_75'\n",
            "ptxas info    : Function properties for _Z8upsamplePKfPfi\n",
            "    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "ptxas info    : Used 9 registers, 372 bytes cmem[0]\n",
            "ptxas info    : Compiling entry function '_Z7maxpoolPKfPfi' for 'sm_75'\n",
            "ptxas info    : Function properties for _Z7maxpoolPKfPfi\n",
            "    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "ptxas info    : Used 14 registers, 372 bytes cmem[0]\n",
            "ptxas info    : Compiling entry function '_Z9conv_reluPKfS0_Pfiiii' for 'sm_75'\n",
            "ptxas info    : Function properties for _Z9conv_reluPKfS0_Pfiiii\n",
            "    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "ptxas info    : Used 40 registers, 392 bytes cmem[0]\n",
            "[GPU] U-Net forward pass time: 0.948448 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wzLI_IXCXQKh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}